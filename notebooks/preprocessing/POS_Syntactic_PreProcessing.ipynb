{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c686944-cf17-4b77-83e1-e6bc67b0ce01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26810c209630425abbb4d24a80152b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 20:47:45 INFO: Downloaded file to C:\\Users\\Saad\\stanza_resources\\resources.json\n",
      "2025-11-10 20:47:45 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2025-11-10 20:47:45 INFO: File exists: C:\\Users\\Saad\\stanza_resources\\ar\\default.zip\n",
      "2025-11-10 20:47:47 INFO: Finished downloading models and saved to C:\\Users\\Saad\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('ar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54210b1c-9125-44d2-a6f0-ee76522c43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "times = []\n",
    "Beginning = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8418905c-55ef-4726-a3db-34744c34e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pandas as pd, numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pandarallel import pandarallel\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import os\n",
    "import stanza\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6ffcf4-5d2a-48b9-9f42-a704d3fc698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Saad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5f1ed7-58f6-4757-a542-15641e5e75a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 1 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n",
      "\n",
      "Using 1 cores for parallel processing\n"
     ]
    }
   ],
   "source": [
    "# n_workers = max(1, os.cpu_count() // 4)\n",
    "\n",
    "n_workers = 3\n",
    "pandarallel.initialize(nb_workers=n_workers, progress_bar=True)\n",
    "print(f\"\\nUsing {n_workers} cores for parallel processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b0a54b0-b10c-42e8-81fc-bcf7a2635b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data …\n",
      "Loaded 10,000 rows\n"
     ]
    }
   ],
   "source": [
    "TARGET = 10000\n",
    "MinimumArabicPercentage = 0.4\n",
    "\n",
    "print('Loading data …')\n",
    "df = pd.read_csv('arabic_sentiment_reviews.csv', encoding='utf-8', nrows=TARGET)\n",
    "print(f'Loaded {len(df):,} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d46da3-2520-4525-8bed-aeecdcf28960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Arabic-ratio filter …\n",
      "Kept 10,000 rows after filter\n"
     ]
    }
   ],
   "source": [
    "def arabic_ratio(text: str) -> float:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "    arabic_len = sum(1 for ch in text if '\\u0600' <= ch <= '\\u06FF')\n",
    "    return arabic_len / max(len(text), 1)\n",
    "\n",
    "print('Applying Arabic-ratio filter …')\n",
    "df['arabic_ratio'] = df['content'].apply(arabic_ratio)\n",
    "print(f'Kept {len(df):,} rows after filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5727e465-d0cb-4053-9151-134c04799a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_arabic_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', ' ', text)   # drop non-Arabic\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"و\", text)\n",
    "    text = re.sub(\"ئ\", \"ي\", text)\n",
    "    text = re.sub(\"ء\", \"\", text)   # drop hamza\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23c0148-c44a-42c5-ab80-a76168badbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating character 3‑grams and 4‑grams (non‑parallel) …\n",
      "Character n‑grams generated in 1.2 s\n"
     ]
    }
   ],
   "source": [
    "def get_char_ngrams(text: str, n: int) -> str:\n",
    "    \"\"\"Generate space‑separated character n‑grams from cleaned text.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return ''\n",
    "    \n",
    "    # Re‑use the same cleaning logic for consistency\n",
    "    cleaned = clean_arabic_text(text)\n",
    "    \n",
    "    # Skip if cleaned text is shorter than n\n",
    "    if len(cleaned) < n:\n",
    "        return ''\n",
    "    \n",
    "    # Sliding‑window n‑grams\n",
    "    ngrams = [cleaned[i:i+n] for i in range(len(cleaned) - n + 1)]\n",
    "    return ' '.join(ngrams)\n",
    "\n",
    "print('Generating character 3‑grams and 4‑grams (non‑parallel) …')\n",
    "start = time.time()\n",
    "df['char_3grams'] = df['content'].apply(lambda x: get_char_ngrams(x, 3))\n",
    "df['char_4grams'] = df['content'].apply(lambda x: get_char_ngrams(x, 4))\n",
    "elapsed = time.time() - start\n",
    "times.append([\" Caharacter n- grams: \", elapsed])\n",
    "print(f'Character n‑grams generated in {elapsed:.1f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9384c486-4348-4736-be4e-4517ea0caafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1beb28e6b664018871b965e46ff160e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=10000), Label(value='0 / 10000')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming finished in 5.1 s  (10,000 rows)\n"
     ]
    }
   ],
   "source": [
    "def light_stem(txt: str) -> str:\n",
    "    import re\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem.isri import ISRIStemmer  # ← import here\n",
    "    stemmer = ISRIStemmer()                 # ← instantiate here\n",
    "\n",
    "    if not isinstance(txt, str) or not txt.strip():\n",
    "        return ''\n",
    "    txt = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', ' ', txt)   # drop non-Arabic\n",
    "    txt = re.sub(r'\\\\s+', ' ', txt).strip()\n",
    "    txt = re.sub(\"[إأٱآا]\", \"ا\", txt)\n",
    "    txt = re.sub(\"ة\", \"ه\", txt)\n",
    "    txt = re.sub(\"ى\", \"ي\", txt)\n",
    "    txt = re.sub(\"ؤ\", \"و\", txt)\n",
    "    txt = re.sub(\"ئ\", \"ي\", txt)\n",
    "    txt = re.sub(\"ء\", \"\", txt)   # drop hamza\n",
    "    txt = re.sub(r'(.)\\1{2,}', r'\\1\\1', txt)\n",
    "\n",
    "    tokens = word_tokenize(txt)\n",
    "    \n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(stems)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "df['text_stem'] = df['content'].parallel_apply(light_stem)\n",
    "elapsed = time.time() - start\n",
    "times.append([\" Stemming ISRI: \", elapsed])\n",
    "print(f'Stemming finished in {elapsed:.1f} s  ({len(df):,} rows)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2947e07f-58a0-4ba0-a9a6-bf0a1c6e4c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel POS & NER tagging...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494ea5d1d92a4d64b8e95ada8858da55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=10000), Label(value='0 / 10000')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MaybeEncodingError",
     "evalue": "Error sending result: '<multiprocessing.pool.ExceptionWithTraceback object at 0x000001541E121C10>'. Reason: 'TypeError(\"cannot pickle '_thread.lock' object\")'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMaybeEncodingError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting parallel POS & NER tagging...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m start_stanza \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 44\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_tagged\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdep_tagged\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mparallel_apply(process_stanza_parallel)\n\u001b[0;32m     46\u001b[0m elapsed_stanza \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_stanza\n\u001b[0;32m     47\u001b[0m times\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Tagging POS & Deppar Stanza: \u001b[39m\u001b[38;5;124m\"\u001b[39m, elapsed_stanza])\n",
      "File \u001b[1;32mD:\\Users\\Saad\\anaconda3\\Lib\\site-packages\\pandarallel\\core.py:444\u001b[0m, in \u001b[0;36mparallelize_with_pipe.<locals>.closure\u001b[1;34m(data, user_defined_function, *user_defined_function_args, **user_defined_function_kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m worker_status \u001b[38;5;241m==\u001b[39m WorkerStatus\u001b[38;5;241m.\u001b[39mError:\n\u001b[0;32m    442\u001b[0m         progress_bars\u001b[38;5;241m.\u001b[39mset_error(worker_index)\n\u001b[1;32m--> 444\u001b[0m results \u001b[38;5;241m=\u001b[39m results_promise\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_type\u001b[38;5;241m.\u001b[39mreduce(results, reduce_extra)\n",
      "File \u001b[1;32mD:\\Users\\Saad\\anaconda3\\Lib\\multiprocessing\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mMaybeEncodingError\u001b[0m: Error sending result: '<multiprocessing.pool.ExceptionWithTraceback object at 0x000001541E121C10>'. Reason: 'TypeError(\"cannot pickle '_thread.lock' object\")'"
     ]
    }
   ],
   "source": [
    "def process_stanza_parallel(text: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Initializes a single pipeline for POS and NER and returns both.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    if 'stanza_nlp_pipeline_full' not in globals():\n",
    "        \n",
    "        import stanza  \n",
    "        print(\"Initializing combined POS/NER pipeline for a new worker...\")\n",
    "        globals()['stanza_nlp_pipeline_full'] = stanza.Pipeline(\n",
    "            'ar', \n",
    "            processors='tokenize,pos,depparse',  \n",
    "            verbose=False,\n",
    "            download_method=None,\n",
    "            use_gpu=True\n",
    "        )\n",
    "    \n",
    "    nlp = globals()['stanza_nlp_pipeline_full']\n",
    "    \n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return pd.Series(['', ''], index=['pos_tagged', 'dep_tagged'])\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tok_pos_list = []\n",
    "    tok_dep_list = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            tok_pos_list.append(f'{word.text}_{word.upos}')\n",
    "            \n",
    "            head_text = sent.words[word.head-1].text if word.head > 0 else 'ROOT'\n",
    "            # Format: word_relation_head\n",
    "            tok_dep_list.append(f'{word.text}_{word.deprel}_{head_text}')\n",
    "\n",
    "    \n",
    "    return pd.Series([\n",
    "        '|'.join(tok_pos_list),\n",
    "        '|'.join(tok_dep_list)\n",
    "    ], index=['pos_tagged', 'dep_tagged'])\n",
    "\n",
    "print('Starting parallel POS & NER tagging...')\n",
    "start_stanza = time.time()\n",
    "\n",
    "df[['pos_tagged', 'dep_tagged']] = df['content'].parallel_apply(process_stanza_parallel)\n",
    "\n",
    "elapsed_stanza = time.time() - start_stanza\n",
    "times.append([\" Tagging POS & Deppar Stanza: \", elapsed_stanza])\n",
    "print(f'Stanza (POS & NER) tagging finished in {elapsed_stanza:.1f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a56338-878c-48f6-834d-7c7c63fb8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['content'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242170b-da7d-4339-8eaa-f3ac6094e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a5372-74bf-4e72-85c4-c79168600e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "# Pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(\n",
    "    label_counts,\n",
    "    labels=label_counts.index.map({0: 'Negative', 1: 'Positive'}),  # optional label names\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=['#ff9999', '#66b3ff']  # optional colors\n",
    ")\n",
    "\n",
    "plt.title('Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d30149-33a7-44c0-a00f-7968a3e144f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = 'arabic_pos_syntactic_tagged.csv'\n",
    "df.to_csv(out_file, index=False, encoding='utf-8')\n",
    "print(f'Saved → {out_file}  ({len(df):,} rows)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c798078-96b2-4329-9b00-2e590150d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = time.time()\n",
    "times.append([\"Overall\", Final - Beginning])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d966689-af32-45ae-9c00-447120a06459",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"n_rows: {TARGET}\\n\")\n",
    "print(\"Times for POS_Tagging Processing\")\n",
    "for i in times:\n",
    "    print(f\"⏱️ {i[0]} : {i[1]:.6} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
