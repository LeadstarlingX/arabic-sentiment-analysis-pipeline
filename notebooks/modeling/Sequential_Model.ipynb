{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a50350d-5b92-4be2-b65b-45cf58dd5ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1225d86f-ec54-429b-963f-a1a520046953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('arabic_reviews_sequential_clean.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ce9e1c-7758-4716-98f1-374788298af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>lemmatized_content</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>fourgrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['نعال', 'مريحه', 'رتد', 'نعال', 'هي', 'دافئ',...</td>\n",
       "      <td>['نعال_مريحه', 'مريحه_رتد', 'رتد_نعال', 'نعال_...</td>\n",
       "      <td>['نعال_مريحه_رتد', 'مريحه_رتد_نعال', 'رتد_نعال...</td>\n",
       "      <td>['نعال_مريحه_رتد_نعال', 'مريحه_رتد_نعال_هي', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['منتج', 'جميل', 'خدم', 'ئه', 'قد', 'اشتري', '...</td>\n",
       "      <td>['منتج_جميل', 'جميل_خدم', 'خدم_ئه', 'ئه_قد', '...</td>\n",
       "      <td>['منتج_جميل_خدم', 'جميل_خدم_ئه', 'خدم_ئه_قد', ...</td>\n",
       "      <td>['منتج_جميل_خدم_ئه', 'جميل_خدم_ئه_قد', 'خدم_ئه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>['جيد', 'اشياء', 'صغيره', 'عمل', 'شكل', 'جيد',...</td>\n",
       "      <td>['جيد_اشياء', 'اشياء_صغيره', 'صغيره_عمل', 'عمل...</td>\n",
       "      <td>['جيد_اشياء_صغيره', 'اشياء_صغيره_عمل', 'صغيره_...</td>\n",
       "      <td>['جيد_اشياء_صغيره_عمل', 'اشياء_صغيره_عمل_شكل',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>['هي', 'غايه', 'غايه', 'نت', 'شتر', 'حذر', 'جد...</td>\n",
       "      <td>['هي_غايه', 'غايه_غايه', 'غايه_نت', 'نت_شتر', ...</td>\n",
       "      <td>['هي_غايه_غايه', 'غايه_غايه_نت', 'غايه_نت_شتر'...</td>\n",
       "      <td>['هي_غايه_غايه_نت', 'غايه_غايه_نت_شتر', 'غايه_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>['اشخاص', 'حب', 'ضحك', 'قط', 'ان', 'بتسم', 'عن...</td>\n",
       "      <td>['اشخاص_حب', 'حب_ضحك', 'ضحك_قط', 'قط_ان', 'ان_...</td>\n",
       "      <td>['اشخاص_حب_ضحك', 'حب_ضحك_قط', 'ضحك_قط_ان', 'قط...</td>\n",
       "      <td>['اشخاص_حب_ضحك_قط', 'حب_ضحك_قط_ان', 'ضحك_قط_ان...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                 lemmatized_content  \\\n",
       "0      1  ['نعال', 'مريحه', 'رتد', 'نعال', 'هي', 'دافئ',...   \n",
       "1      1  ['منتج', 'جميل', 'خدم', 'ئه', 'قد', 'اشتري', '...   \n",
       "2      1  ['جيد', 'اشياء', 'صغيره', 'عمل', 'شكل', 'جيد',...   \n",
       "3      0  ['هي', 'غايه', 'غايه', 'نت', 'شتر', 'حذر', 'جد...   \n",
       "4      1  ['اشخاص', 'حب', 'ضحك', 'قط', 'ان', 'بتسم', 'عن...   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  ['نعال_مريحه', 'مريحه_رتد', 'رتد_نعال', 'نعال_...   \n",
       "1  ['منتج_جميل', 'جميل_خدم', 'خدم_ئه', 'ئه_قد', '...   \n",
       "2  ['جيد_اشياء', 'اشياء_صغيره', 'صغيره_عمل', 'عمل...   \n",
       "3  ['هي_غايه', 'غايه_غايه', 'غايه_نت', 'نت_شتر', ...   \n",
       "4  ['اشخاص_حب', 'حب_ضحك', 'ضحك_قط', 'قط_ان', 'ان_...   \n",
       "\n",
       "                                            trigrams  \\\n",
       "0  ['نعال_مريحه_رتد', 'مريحه_رتد_نعال', 'رتد_نعال...   \n",
       "1  ['منتج_جميل_خدم', 'جميل_خدم_ئه', 'خدم_ئه_قد', ...   \n",
       "2  ['جيد_اشياء_صغيره', 'اشياء_صغيره_عمل', 'صغيره_...   \n",
       "3  ['هي_غايه_غايه', 'غايه_غايه_نت', 'غايه_نت_شتر'...   \n",
       "4  ['اشخاص_حب_ضحك', 'حب_ضحك_قط', 'ضحك_قط_ان', 'قط...   \n",
       "\n",
       "                                           fourgrams  \n",
       "0  ['نعال_مريحه_رتد_نعال', 'مريحه_رتد_نعال_هي', '...  \n",
       "1  ['منتج_جميل_خدم_ئه', 'جميل_خدم_ئه_قد', 'خدم_ئه...  \n",
       "2  ['جيد_اشياء_صغيره_عمل', 'اشياء_صغيره_عمل_شكل',...  \n",
       "3  ['هي_غايه_غايه_نت', 'غايه_غايه_نت_شتر', 'غايه_...  \n",
       "4  ['اشخاص_حب_ضحك_قط', 'حب_ضحك_قط_ان', 'ضحك_قط_ان...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe629a2-af83-4c23-a085-af2fe722504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = ['lemmatized_content', 'bigrams', 'trigrams', 'fourgrams']\n",
    "\n",
    "# Use .apply() with ast.literal_eval to safely convert them back\n",
    "for col in list_columns:\n",
    "    df[col] = df[col].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e26c71-a6ff-4f47-871c-a425fdd3c580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'نعال_مريحه'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bigrams'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1e334d-fec0-4157-a82e-61cc8c077c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['lemmatized_content', 'bigrams', 'trigrams']\n",
    "\n",
    "X = df[features]\n",
    "y = df['label']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a1c04e6-1d6b-404f-8da1-98ff40ae16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_len_feats(X):\n",
    "    # Apply 'len' directly to the list to get the word count\n",
    "    word_cnt = X['lemmatized_content'].apply(len)\n",
    "    \n",
    "    # Join the list of words into a string, then get the string length\n",
    "    txt_len = X['lemmatized_content'].apply(lambda x: len(\" \".join(x)))\n",
    "    \n",
    "    return X.assign(\n",
    "        txt_len=txt_len,\n",
    "        word_cnt=word_cnt\n",
    "    )\n",
    "\n",
    "X_train = add_len_feats(X_train)\n",
    "X_test = add_len_feats(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da15288-73de-42e5-a7a6-207bf302127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenizer(text):\n",
    "    \"\"\"Simple tokenizer that splits on whitespace\"\"\"\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return []\n",
    "    return str(text).split()\n",
    "\n",
    "def comma_tokenizer(text):\n",
    "    \"\"\"Simple tokenizer that splits on the | character\"\"\"\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return []\n",
    "    return str(text).split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7203448c-5fbf-4c0c-b8f9-107633f9563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple tokenizer that returns the list of tokens it receives\n",
    "def identity_tokenizer(tokens):\n",
    "    return tokens\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('lem', TfidfVectorizer(max_features=30000,\n",
    "            # ngram_range=(1, 2),  <--- REMOVED: No longer needed\n",
    "            min_df=1,\n",
    "            strip_accents=None,\n",
    "            lowercase=False,\n",
    "            tokenizer=identity_tokenizer,  # <--- FIXED: Use identity tokenizer\n",
    "            token_pattern=None ), 'lemmatized_content'),\n",
    "        \n",
    "        ('bi', TfidfVectorizer(max_features=10000,\n",
    "            tokenizer=identity_tokenizer,  # <--- FIXED: Use identity tokenizer\n",
    "            token_pattern=None,\n",
    "            lowercase=False,\n",
    "            min_df=1), 'bigrams'),\n",
    "        \n",
    "        ('tri', TfidfVectorizer(max_features=10000,\n",
    "            tokenizer=identity_tokenizer,  # <--- FIXED: Use identity tokenizer\n",
    "            token_pattern=None,\n",
    "            lowercase=False,\n",
    "            min_df=1), 'trigrams'),\n",
    "        \n",
    "        ('len_feats', StandardScaler(), ['txt_len', 'word_cnt'])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46758902-fafe-42ca-96f6-54b19f5b93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'KNN': (KNeighborsClassifier(), {'model__n_neighbors': [3, 5, 7]}),\n",
    "    'SVM': (SVC(), {'model__C': [0.1, 1, 10], 'model__kernel': ['linear', 'rbf']}),\n",
    "    'LR': (LogisticRegression(max_iter=1000), {'model__C': [0.1, 1, 10]})\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d162ebc4-ae30-48e4-bafd-f6f505ceaa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN...\n",
      "KNN Best Score: 0.5689608357280835\n",
      "KNN Training Time: 6.25 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.52      0.55      1013\n",
      "           1       0.56      0.64      0.60       983\n",
      "\n",
      "    accuracy                           0.58      1996\n",
      "   macro avg       0.58      0.58      0.58      1996\n",
      "weighted avg       0.58      0.58      0.58      1996\n",
      "\n",
      "Training SVM...\n",
      "SVM Best Score: 0.8275092844169923\n",
      "SVM Training Time: 53.35 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85      1013\n",
      "           1       0.84      0.84      0.84       983\n",
      "\n",
      "    accuracy                           0.84      1996\n",
      "   macro avg       0.84      0.84      0.84      1996\n",
      "weighted avg       0.84      0.84      0.84      1996\n",
      "\n",
      "Training LR...\n",
      "LR Best Score: 0.8369042331719314\n",
      "LR Training Time: 4.83 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      1013\n",
      "           1       0.85      0.85      0.85       983\n",
      "\n",
      "    accuracy                           0.85      1996\n",
      "   macro avg       0.85      0.85      0.85      1996\n",
      "weighted avg       0.85      0.85      0.85      1996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, (model, params) in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('prep', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    start_time = time.time()\n",
    "    clf = GridSearchCV(pipe, params, cv=2, scoring='accuracy', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = time.time()  # End timing\n",
    "    \n",
    "    print(f\"{name} Best Score: {clf.best_score_}\")\n",
    "    print(f\"{name} Training Time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    if clf.best_score_ > best_score:\n",
    "        best_score = clf.best_score_\n",
    "        best_model = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb3a34a-1ec6-4eec-bdda-5b4d2edfc974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best model and preprocessor\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "joblib.dump(best_model.best_estimator_.named_steps['prep'], 'vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
